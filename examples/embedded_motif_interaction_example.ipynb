{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFIM: Recovering interactions between embedded motifs\n",
    "\n",
    "In this example notebook we will recover interactions between embedded motifs. In real DNA sequences many features - such as DNA binding motifs - interact or depend on each other to regulate gene expression. Traditionally we can identify these motifs in a sequence, but their relationship to each other - such as whether they work together or not - is more difficult to untangle. DFIM is a method to identify learned interactions between features from a deep learning model and we apply it here to simulated sequences where we have embedded known motifs into a random background so that we can see how the method works in a case study where we know ground truth (more difficult for real data!). \n",
    "\n",
    "We have simulated three sets sequences:\n",
    "\n",
    "- Class 1: 20,000 sequences with motif A (the SIX5 motif) embedded 1-2 times\n",
    "- Class 2: 20,000 sequences with motif B (the ELF1 motif) embedded 1-2 times\n",
    "- Class 3: 20,000 sequences with motif A and motif B (SIX5 and ELF1) each embedded 1-2 times\n",
    "\n",
    "\n",
    "We have pre-trained a model learned to only predict a positive label when both motifs are present. Class 3 sequences with both motifs are positive and Class 1 and 2 with one or the other are both negative. The model must learn the interaction between motif A and B to make correct predictions. We also randomly add in motifs C and D (AP1 and TAL1) 0, 1, or 2 times into all 60,000 sequences to further simulate real sequences where there may be many repeated patterns that do not necessarily relate to the prediction task at hand.\n",
    "\n",
    "Knowing the ground truth, we show that DFIM recovers an interaction between motifs A and B in Class 3 sequences but not between any other of the embedded motifs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 5: GeForce GTX TITAN X (CNMeM is disabled, cuDNN 5105)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg') \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle\n",
    "import itertools\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from Bio import SeqIO\n",
    "\n",
    "import dfim\n",
    "import dfim.util\n",
    "import dfim.core\n",
    "import dfim.null_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify data files\n",
    "\n",
    "labels_file = 'embedded_motif_interaction_data/labels.txt.gz'\n",
    "fasta_file = 'embedded_motif_interaction_data/sequences.fa.gz'\n",
    "simdata_file = 'embedded_motif_interaction_data/simulation_metadata.txt.gz'\n",
    "\n",
    "model_weights = 'embedded_motif_interaction_data/model_weights.h5'\n",
    "model_architecture = 'embedded_motif_interaction_data/model_architecture.json'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data and model\n",
    "In the first part of this example we will:\n",
    "- Load the simulated DNA sequences\n",
    "- Load a trained model using Keras\n",
    "- Get the sequences our model correctly predicted\n",
    "- Identify the locations in these sequences where our motifs were embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sequences\n",
    "\n",
    "fasta_sequences = SeqIO.parse(gzip.open(fasta_file),'fasta')\n",
    "\n",
    "sequence_list = []\n",
    "seq_fasta_list = []\n",
    "\n",
    "for fasta in fasta_sequences:\n",
    "    name, sequence = fasta.id, str(fasta.seq)\n",
    "    seq_fasta_list.append(sequence)\n",
    "    new_sequence = dfim.util.one_hot_encode(sequence)\n",
    "    sequence_list.append(new_sequence)\n",
    "    \n",
    "sequences = np.array(sequence_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model \n",
    "\n",
    "model_json = open(model_architecture, 'r').read()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights(model_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all correctly predicted positive sequences\n",
    "\n",
    "all_labels = pd.read_table(labels_file)\n",
    "all_predictions = model.predict(sequences)\n",
    "\n",
    "true_labels = all_labels.iloc[:, 3].tolist()\n",
    "predicted_labels = all_predictions[:, 0].tolist()\n",
    "\n",
    "correct_pred_list = dfim.util.get_correct_predictions(true_labels, predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset to 1,0000 Class 3 sequences to speed the example\n",
    "\n",
    "compute_range = range(50000, 51000)\n",
    "compute_index = [el for el in compute_range if el in correct_pred_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recover known locations of motifs\n",
    "\n",
    "mutant_loc_dict = dfim.util.process_locations_from_simdata(sequences, simdata_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing DFIM \n",
    "\n",
    "Now we're going to compute DFIM on our sequences with the following procedure:\n",
    "1. Compute mutated sequences at each motif location\n",
    "2. Compute the change in prediction for each of these disruptions\n",
    "3. Compute the importance scores for each original and mutated sequence\n",
    "4. Compute \"delta profiles\" between mutated and original sequences\n",
    "5. Combine these delta profiles into sequence-level interactions maps of all motifs in the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating mutated sequences\n",
      "Calculating Importance Scores\n",
      "nonlinear_mxts_mode is set to: Gradient\n",
      "Heads-up: current implementation assumes maxpool layer is followed by a linear transformation (conv/dense layer)\n",
      "No reference provided - using zeros\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n"
     ]
    }
   ],
   "source": [
    "# Generate mutated sequences and a key that specifies the index of each \n",
    "# of the sequence and which mutation is present there\n",
    "(mutated_seq_array, mutated_seq_key) = \\\n",
    "            dfim.core.generate_mutants_and_key(sequences, \n",
    "                                               mut_loc_dict=mutant_loc_dict,\n",
    "                                               sequence_index=compute_index)\n",
    "\n",
    "# Look at how these mutations change the model's predictions\n",
    "mutated_seq_preds = model.predict(mutated_seq_array)\n",
    "\n",
    "# Compute importance scores for original and mutated sequences\n",
    "score_dict = dfim.core.compute_importance(model, mutated_seq_array, \n",
    "                                          tasks=[2],\n",
    "                                          score_type='gradient_input',\n",
    "                                          find_scores_layer_idx=0,\n",
    "                                          target_layer_idx=-2)\n",
    "\n",
    "# Compute the change in importance scores as a result of each mutation\n",
    "tasks = [2]\n",
    "delta_dict = dfim.core.compute_delta_profiles(score_dict, mutated_seq_key, \n",
    "                                              mutant_loc_dict, tasks, \n",
    "                                              compute_index, \n",
    "                                              mutated_seq_preds)\n",
    "\n",
    "\n",
    "# Compute DFIM by taking the maximal total affect across all 4 bases for each position\n",
    "dfim_dict = dfim.core.compute_dfim(delta_dict, compute_index, tasks,\n",
    "                                   operations=[np.sum, np.max], \n",
    "                                   operation_axes=[1, 0],\n",
    "                                   absolute_value=True, annotate=True, \n",
    "                                   mutated_seq_key=mutated_seq_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing DFIM of shuffled sequences for fitting a NULL model\n",
    "\n",
    "We're now going to compute a null distribution of interaction scores by shuffling our input sequences and re-computing DFIM maps. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using random shuffle\n",
      "Generating mutated sequences\n",
      "Calculating Importance Scores\n",
      "nonlinear_mxts_mode is set to: Gradient\n",
      "Heads-up: current implementation assumes maxpool layer is followed by a linear transformation (conv/dense layer)\n",
      "No reference provided - using zeros\n",
      "Done 0\n",
      "Done 1000\n",
      "Done 2000\n",
      "Done 3000\n"
     ]
    }
   ],
   "source": [
    "# Shuffle sequences\n",
    "shuf_sequences = dfim.null_model.shuffle_seq_from_one_hot(sequences, dinuc=False)\n",
    "\n",
    "\n",
    "# Repeat DFIM procedure\n",
    "(shuf_mutated_seq_array, shuf_mutated_seq_key) = \\\n",
    "            dfim.core.generate_mutants_and_key(shuf_sequences, \n",
    "                                               mut_loc_dict=mutant_loc_dict,\n",
    "                                               sequence_index=compute_index)\n",
    "    \n",
    "shuf_mutated_seq_preds = model.predict(shuf_mutated_seq_array)\n",
    "\n",
    "shuf_score_dict = dfim.core.compute_importance(model, shuf_mutated_seq_array, \n",
    "                                               tasks=[2],\n",
    "                                               score_type='gradient_input',\n",
    "                                               find_scores_layer_idx=0,\n",
    "                                               target_layer_idx=-2)\n",
    "\n",
    "shuf_delta_dict = dfim.core.compute_delta_profiles(shuf_score_dict, shuf_mutated_seq_key, \n",
    "                                                   mutant_loc_dict, tasks, compute_index, \n",
    "                                                   shuf_mutated_seq_preds)\n",
    "\n",
    "\n",
    "shuf_dfim_dict = dfim.core.compute_dfim(shuf_delta_dict, compute_index, tasks,\n",
    "                                        operations=[np.sum, np.max], \n",
    "                                        operation_axes=[1, 0],\n",
    "                                        absolute_value=True, annotate=True, \n",
    "                                        mutated_seq_key=shuf_mutated_seq_key)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DFIM package enables p-values to be computed in two ways. The default is to \"fit\" a NULL distribution as opposed to using empirical p-values. With a sufficient number of sequences we can fit a Gaussian distribution to the distribution of DFIM scores for the NULL model and assign a p-value to each true score according to this distribution. This is the (null_type = 'fit') p-value. With insufficient data points, we can otherwise assign an empirical p-value according to how many interaction scores in the NULL distribution the true interaction scores are greater than (null_type = 'empirical').\n",
    "\n",
    "We also set the distribution NULL p-values on several levels: by individual sequence or map (null_level='per_map'), by each task (null_level='per_task'), or for all values in the analysis (null_level='global'). This determines what values the Gaussian will be fit with. The default is 'per_task' but if sequences are highly variable (such as widely ranging GC content, etc.) it can be better to fit 'per_map' or if you don't have enough sequences and want to use all data for your NULL then 'global' can be a good option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compute Pvalues \n",
    "\n",
    "all_motifs = ['TAL1', 'ELF1', 'SIX5', 'AP1']\n",
    "\n",
    "dfim_pval_dict = dfim.null_model.assign_pval(dfim_dict, \n",
    "                                             shuf_dfim_dict,\n",
    "                                             null_level = 'per_task',\n",
    "                                             null_type = 'fit')\n",
    "\n",
    "\n",
    "interact_score_dict = {'_'.join(sorted([m1, m2])): [] \n",
    "                           for (m1, m2) \n",
    "                           in itertools.combinations(all_motifs, 2)} \n",
    "\n",
    "for task in dfim_pval_dict.keys():\n",
    "    for seq in dfim_pval_dict[task].keys():\n",
    "        for i in dfim_pval_dict[task][seq].index:\n",
    "            \n",
    "            row_motif = i.split('_')[1]\n",
    "            \n",
    "            for c in dfim_pval_dict[task][seq].columns:\n",
    "                col_motif = c.split('_')[1]\n",
    "                \n",
    "                if i == c:\n",
    "                    continue\n",
    "                else:\n",
    "                    interact_score_dict['_'.join(sorted([row_motif, col_motif]))].append(\n",
    "                                                 dfim_pval_dict[task][seq].loc[i, c])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize learned interactions\n",
    "\n",
    "Now we can visualize our results. We can look at how the p-values of the interaction scores for our embedded motifs that interact are highly significant, while all other interactions between embedded motifs are not. Cool! You can see how this might be applied in real DNA sequences  where there are many motifs present, but only a subset of them interact. DFIM can help identify those interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline  \n",
    "\n",
    "all_data = [interact_score_dict[perm] \n",
    "                for perm in interact_score_dict.keys()]\n",
    "labels = interact_score_dict.keys()\n",
    "tick_labels = [' and '.join([l.split('_')[0], \n",
    "                             l.split('_')[1]]) \n",
    "                       for l in labels]\n",
    "\n",
    "plt.figure(figsize=(10,8))\n",
    "bplot1 = plt.boxplot(all_data, patch_artist=True,\n",
    "                     labels=tick_labels, \n",
    "                     showfliers=False,\n",
    "                     widths=0.5,\n",
    "                     vert=False)\n",
    "plt.tick_params(labelsize=14)\n",
    "plt.axvline(0.05, ls='dashed')\n",
    "plt.xticks(rotation=45, fontsize=16)\n",
    "plt.xlabel('p-value', fontsize=16)\n",
    "colors = ['lightblue']*len(tick_labels)\n",
    "colors[-2] = 'lightgreen'\n",
    "for patch, color in zip(bplot1['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
